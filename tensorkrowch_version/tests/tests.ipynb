{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e6e728",
   "metadata": {},
   "source": [
    "# MPS Hamiltonian Learning with TensorKrowch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "d30c3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import tensorkrowch as tk\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ea603",
   "metadata": {},
   "source": [
    "### Things to do:\n",
    "\n",
    "This will be a pytorch version of the Hamiltonian learning problem, where the NN has a layer with an MPS structure from tensorkrowch. There are a few things that I need to sort out:\n",
    "1. This NN doesn't train from data, it starts from an ansatz and modifies it until it finds the optimal solution\n",
    "2. I still need to run the dynamics for every epoch, which consist on:\n",
    "    2.1. Taking initial state and applyting rotations in the X,Y and Z directions, with the option to customize which rotations I want to apply\n",
    "    2.2. Doing a time evolution of the resulting state under a Hamiltonian. The Hamiltonian contains only interaction terms, and it also must be customizable\n",
    "3. After running the Hamiltonian, we extract bitstring probabilities and compute nll loss function with the input data, which are the generated bitstrings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596daf3",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "551129c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    '''Load configuration from YAML file'''\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def load_experimental_data(config):\n",
    "    \"\"\"Load experimental/simulated data\"\"\"\n",
    "    N = config[\"L\"]\n",
    "    chi = config['bond_dimension']\n",
    "    T_max = config[\"t_max\"]\n",
    "    search_pattern = f\"../data/experimental_data_quantum_sampling_L{N}_Chi_{chi}_*_counts.csv\"\n",
    "    files = glob.glob(search_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No data found for L={N}\")\n",
    "\n",
    "    config_file = files[0]\n",
    "    file_core = config_file.replace(\".csv\", \"\").replace(\"../data/experimental_data_quantum_sampling_\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LOADING DATA: {file_core}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df_counts = pd.read_csv(f\"../data/experimental_data_quantum_sampling_{file_core}.csv\")\n",
    "        \n",
    "    # Remove leading single quote if present\n",
    "    if df_counts['bitstring'].astype(str).str.startswith(\"'\").all():\n",
    "        df_counts['bitstring'] = df_counts['bitstring'].str[1:]\n",
    "    \n",
    "    # Now extract values\n",
    "    bitstrings = df_counts['bitstring'].values.astype(str)\n",
    "    counts_shots = df_counts['count'].values.astype(np.int32)\n",
    "    \n",
    "    return bitstrings, counts_shots\n",
    "\n",
    "\n",
    "def local_probability_tensor(strings, counts):\n",
    "    '''Calculates probabilities of each qubit, returning vector of size L\n",
    "    containing the probs of each qubit being 0 or 1'''\n",
    "    L = len(strings[0])\n",
    "    total_counts = sum(counts)\n",
    "    \n",
    "    prob_matrix = torch.zeros((1, L, 2)) #First index is batch. Needed for feeding into NN\n",
    "    \n",
    "    for bitstring, count in zip(strings, counts):\n",
    "        for qubit in range(L):\n",
    "            bit_value = int(bitstring[qubit])\n",
    "            prob_matrix[0, qubit, bit_value] += count\n",
    "    \n",
    "    # Normalize by total counts\n",
    "    prob_matrix /= total_counts\n",
    "    \n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f031b6",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "7c36a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paulis(dtype=torch.complex64):\n",
    "    '''Creates single-qubit basis operators'''\n",
    "    sx = torch.tensor([[0., 1.], [1., 0.]], dtype=dtype)\n",
    "    sy = torch.tensor([[0., -1j], [1j, 0.]], dtype=dtype)\n",
    "    sz = torch.tensor([[1., 0.], [0., -1.]], dtype=dtype)\n",
    "    id2 = torch.eye(2, dtype=dtype)\n",
    "    return sx, sy, sz, id2\n",
    "\n",
    "def kron_n(ops):\n",
    "    '''Tensor product of a list of operators'''\n",
    "    out = ops[0]\n",
    "    for A in ops[1:]:\n",
    "        out = torch.kron(out, A)\n",
    "    return out\n",
    "\n",
    "\n",
    "def x_rotation(theta, dtype=torch.complex64):\n",
    "    sx = torch.tensor([[0., 1.], [1., 0.]], dtype=dtype)\n",
    "    return torch.matrix_exp(-1j * theta / 2 * sx)\n",
    "\n",
    "def y_rotation(theta, dtype=torch.complex64):\n",
    "    sy = torch.tensor([[0., -1j], [1j, 0.]], dtype=dtype)\n",
    "    return torch.matrix_exp(-1j * theta / 2 * sy)\n",
    "\n",
    "def z_rotation(theta, dtype=torch.complex64):\n",
    "    sz = torch.tensor([[1., 0.], [0., -1.]], dtype=dtype)\n",
    "    return torch.matrix_exp(-1j * theta / 2 * sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "78840517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_initial_state(L, kind, dtype=torch.complex64):\n",
    "    \"\"\"Prepare initial quantum states for L qubits.\"\"\"\n",
    "    if kind == 'all_zeros':\n",
    "        psi0 = torch.zeros(2**L, dtype=dtype)\n",
    "        psi0[0] = 1.0\n",
    "        \n",
    "    elif kind == 'all_plus':\n",
    "        plus = torch.ones(2, dtype=dtype) / np.sqrt(2)\n",
    "        psi0 = plus\n",
    "        for _ in range(L - 1):\n",
    "            psi0 = torch.kron(psi0, plus)\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(f\"Initial state '{kind}' not recognized. \"\n",
    "                        f\"Use 'all_zeros' or 'all_plus'\")\n",
    "    return psi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "f949bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperatorClass:\n",
    "    '''Class that contains a list of all the operator types the Hamiltonian will have\n",
    "       The operators will be applied to each qubit, and we will allow for the construction of any\n",
    "       combination of Pauli strings \n",
    "    '''\n",
    "    def __init__(self, L, dtype=torch.complex64):\n",
    "\n",
    "        self.L = L\n",
    "        self.dim = 2**L\n",
    "        self.pauli_basis = {}\n",
    "        self.pauli_basis['X'], self.pauli_basis['Y'], self.pauli_basis['Z'], self.pauli_basis['I'] = paulis(dtype)\n",
    "        self.operators = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.operators)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.operators[idx]\n",
    "    \n",
    "    def add_operators(self, pauli_string:str):\n",
    "        #e.g. 'X','Y','ZZ'\n",
    "        '''Adds one type of operator at a time. It loops through all the qubits, \n",
    "        and for each position does the tensor product of the whole chain, with the \n",
    "        required qubits substituted by the operators of the string'''\n",
    "\n",
    "        if len(pauli_string) > self.L:\n",
    "            raise ValueError(f\"Pauli string '{pauli_string}' longer than system size {self.L}\")\n",
    "        \n",
    "        if not all(char in 'XYZI' for char in pauli_string):\n",
    "            raise ValueError(f\"Invalid character in '{pauli_string}'. Use only X, Y, Z, I\")\n",
    "        \n",
    "        for i in range(self.L - len(pauli_string) + 1):\n",
    "                #Create identity operators for each qubit\n",
    "                ops = [self.pauli_basis['I']]*self.L\n",
    "                for j, char in enumerate(pauli_string):\n",
    "                     #Build string\n",
    "                     ops[i+j] = self.pauli_basis[char]\n",
    "                self.operators.append(kron_n(ops))\n",
    "        print(f\"{pauli_string} terms added to the Hamiltonian\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11a645",
   "metadata": {},
   "source": [
    "## Manual NN\n",
    "\n",
    "These were the functions defined by Marcin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "db2551a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(params, x):\n",
    "    h = x\n",
    "    for layer in params[:-1]:\n",
    "        h = np.tanh(h @ layer[\"W\"] + layer[\"b\"])\n",
    "    last = params[-1]\n",
    "    return h @ last[\"W\"] + last[\"b\"]\n",
    "\n",
    "\n",
    "def init_mlp_params(layer_sizes, scale=0.1):\n",
    "    params = []\n",
    "    for i, (m, n) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        # Initialize weights with scaled normal distribution\n",
    "        W = scale * torch.randn((m, n))\n",
    "        # Initialize biases to zero\n",
    "        b = torch.zeros((n,))\n",
    "        params.append({\"W\": W, \"b\": b})\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3dfc28",
   "metadata": {},
   "source": [
    "## Pytorch NN\n",
    "\n",
    "Equivalent of Marcin's functions but with Pytorch syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "394c583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Create linear layers\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "        \n",
    "        # Custom initialization (similar to your function)\n",
    "        self._initialize_parameters()\n",
    "    \n",
    "    def _initialize_parameters(self, scale=0.1):\n",
    "        \"\"\"Initialize weights with normal distribution and biases to zero.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=scale)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply all but last layer with tanh activation\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = torch.tanh(layer(x))\n",
    "        \n",
    "        # Last layer - linear only (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32e902",
   "metadata": {},
   "source": [
    "## TensorKrowch NN\n",
    "\n",
    "My pytorch NN integrating MPS layer from TensorKrowch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "e09b1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, for input layer I should put number of possible bitstrings, and for output number of parameters to train. In the middle idk yet, but we'll see\n",
    "class MPS_MLP(nn.Module):\n",
    "    def __init__(self, L, chi, num_params, num_dims = []):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        layer_sizes = num_dims + [num_params]\n",
    "\n",
    "        # 1. MPS input layer: processes L×2 features → first hidden size (will be output size if no middle layers)\n",
    "        mps = tk.models.MPSLayer(\n",
    "            n_features=L,\n",
    "            in_dim=2,\n",
    "            out_dim=layer_sizes[0],  \n",
    "            bond_dim=chi\n",
    "        )\n",
    "        self.layers.append(mps)\n",
    "        \n",
    "        # 2. Middle layers (optional)\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            self.layers.append(nn.Linear(\n",
    "                layer_sizes[i],\n",
    "                layer_sizes[i+1]\n",
    "            ))\n",
    "        \n",
    "        if len(layer_sizes) > 1:\n",
    "        # 3. Final output layer: hidden → num_params\n",
    "            self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
    "        \n",
    "        # Custom initialization (similar to your function)\n",
    "        self._initialize_parameters()\n",
    "    \n",
    "    def _initialize_parameters(self, scale=0.1):\n",
    "        \"\"\"Initialize weights with normal distribution and biases to zero.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear): #MPSLayer initializes itself\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=scale)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply all but last layer with tanh activation\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = torch.tanh(layer(x))\n",
    "        \n",
    "        # Last layer - linear only (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "98223c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_rotations_direct(psi, params, L):\n",
    "#     \"\"\"\n",
    "#     Apply rotations by reshaping the state vector and applying 2x2 matrices.\n",
    "#     This is the most efficient method for single-qubit rotations.\n",
    "#     \"\"\"\n",
    "#     # Map parameter keys to rotation functions\n",
    "#     rot_funcs = {\n",
    "#         'rot_x': lambda theta: x_rotation(theta, dtype=psi.dtype),\n",
    "#         'rot_y': lambda theta: y_rotation(theta, dtype=psi.dtype),\n",
    "#         'rot_z': lambda theta: z_rotation(theta, dtype=psi.dtype)\n",
    "#     }\n",
    "    \n",
    "#     # Initialize per-qubit rotations as identity\n",
    "#     per_qubit_rots = [torch.eye(2, dtype=psi.dtype, device=psi.device) for _ in range(L)]\n",
    "    \n",
    "#     # Accumulate all rotations for each qubit\n",
    "#     for key, rot_func in rot_funcs.items():\n",
    "#         if key in params:\n",
    "#             rot_list = params[key]\n",
    "#             for i in range(L):\n",
    "#                 if i < len(rot_list):\n",
    "#                     per_qubit_rots[i] = rot_func(rot_list[i]) @ per_qubit_rots[i]\n",
    "    \n",
    "#     # Apply rotations qubit by qubit using tensor reshaping\n",
    "#     # This avoids constructing the full 2^L × 2^L matrix\n",
    "#     for i in range(L):\n",
    "#         # Reshape state to separate the i-th qubit\n",
    "#         shape = [2] * L\n",
    "#         psi_reshaped = psi.view(*shape)\n",
    "        \n",
    "#         # Move the i-th dimension to the front\n",
    "#         psi_reshaped = psi_reshaped.movedim(i, 0)\n",
    "        \n",
    "#         # Apply 2x2 rotation to each 2-element slice\n",
    "#         original_shape = psi_reshaped.shape\n",
    "#         psi_reshaped = psi_reshaped.view(2, -1)\n",
    "#         psi_reshaped = per_qubit_rots[i] @ psi_reshaped\n",
    "        \n",
    "#         # Restore shape and put dimension back\n",
    "#         psi_reshaped = psi_reshaped.view(original_shape)\n",
    "#         psi_reshaped = psi_reshaped.movedim(0, i)\n",
    "        \n",
    "#         psi = psi_reshaped.reshape(-1)\n",
    "    \n",
    "#     return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43577c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rotations(psi, params, L):\n",
    "    sx, sy, sz, id2 = paulis()\n",
    "\n",
    "    rot_funcs = {\n",
    "    'rot_x': lambda theta: x_rotation(theta, dtype=psi.dtype),\n",
    "    'rot_y': lambda theta: y_rotation(theta, dtype=psi.dtype),\n",
    "    'rot_z': lambda theta: z_rotation(theta, dtype=psi.dtype)\n",
    "    }\n",
    "    \n",
    "    for key, rot_func in rot_funcs.items():\n",
    "        if key in params:\n",
    "            for i in range(L):\n",
    "                rot = kron_n([id2]*i + [rot_func(params[key][i])] + [id2]*(L-i-1))\n",
    "                #print(f'rot {key} in qubit {i} of angle {params[key][i]}')\n",
    "                psi = rot@psi\n",
    "                         \n",
    "    return psi  \n",
    " \n",
    "\n",
    "def rk4_step(state, H, t, dt, rhs_fun):\n",
    "    dt_c = torch.asarray(dt, dtype=state.dtype)\n",
    "    k1 = rhs_fun(H, t, state)\n",
    "    k2 = rhs_fun(H, t + 0.5*dt_c, state + 0.5*dt_c*k1)\n",
    "    k3 = rhs_fun(H, t + 0.5*dt_c, state + 0.5*dt_c*k2)\n",
    "    k4 = rhs_fun(H, t + dt_c, state + dt_c*k3)\n",
    "    state_next = state + (dt_c/6.0)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "    \n",
    "    if state.ndim == 1:  # State vector\n",
    "        norm = torch.linalg.norm(state_next)\n",
    "        return state_next / (norm + 1e-12)\n",
    "    else:  # Density matrix\n",
    "        state_next = 0.5 * (state_next + state_next.conj().T)\n",
    "        trace = torch.trace(state_next).real\n",
    "        return state_next / (trace + 1e-12)\n",
    "\n",
    "\n",
    "def build_hamiltonian(L, theta, OPS_LIST):\n",
    "    '''Creates Hamiltonian from list of operators and corresponding weights'''\n",
    "\n",
    "    expected_shape = len(OPS_LIST)\n",
    "    \n",
    "    if len(theta) != expected_shape or len(OPS_LIST) != expected_shape:\n",
    "        raise ValueError(f\"Parameter/operator count mismatch\")\n",
    "    \n",
    "    H = torch.zeros((2**L, 2**L), dtype=torch.complex64)\n",
    "    for i in range(expected_shape):\n",
    "        H += theta[i] * OPS_LIST.operators[i]\n",
    "    \n",
    "    return H\n",
    "\n",
    "\n",
    "def schrodinger_rhs(H, t, psi):\n",
    "    return -1j * (H @ psi)\n",
    "\n",
    "\n",
    "def evolve_state(psi_t, H, t_grid):\n",
    "    rhs_fun = schrodinger_rhs\n",
    "    dt = t_grid[1] - t_grid[0]\n",
    "    for i,t in enumerate(t_grid[0:-1]):\n",
    "        dt = t_grid[i+1] - t_grid[i]\n",
    "        psi_t = rk4_step(psi_t, H, t, dt, rhs_fun)\n",
    " \n",
    "    return psi_t\n",
    "\n",
    "\n",
    "def time_evolution(psi, theta, OPS_LIST, L, t_grid):\n",
    "\n",
    "    H = build_hamiltonian(L, theta, OPS_LIST)\n",
    "\n",
    "    psi_t = evolve_state(psi, H, t_grid)\n",
    "\n",
    "    return psi_t\n",
    "\n",
    "\n",
    "def physics_computation(params, psi0, OPS_LIST, CONFIG, t_grid):\n",
    "\n",
    "    psi_rot = compute_rotations(psi0, params, CONFIG['L'])\n",
    "\n",
    "    psi_t = time_evolution(psi_rot, params['theta'], OPS_LIST, CONFIG['L'], t_grid)\n",
    "    \n",
    "    return psi_t\n",
    "\n",
    "def nll(psi, counts):\n",
    "    #Propper format of data\n",
    "    counts_torch = torch.from_numpy(counts)\n",
    "\n",
    "    #Normalized probabilities\n",
    "    probs = torch.abs(psi)**2\n",
    "    probs = probs / probs.sum()\n",
    "\n",
    "    #Avoid log(0) by clipping\n",
    "    probs = torch.clip(probs, 1e-9, 1.0)\n",
    "\n",
    "    #negative log likelihood (normalized)\n",
    "    logp = torch.log(probs)\n",
    "    ll = torch.sum(counts_torch * logp)\n",
    "    loss_nll = -ll / torch.sum(counts_torch)\n",
    "\n",
    "    return loss_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "2342d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameter_dict(params, OPS_LIST, CONFIG):\n",
    "    \"\"\"\n",
    "    Create parameter dictionary from model output based on configuration.\n",
    "    \n",
    "    Args:\n",
    "        params: Tensor of shape (total_params,)\n",
    "        OPS_LIST: List of Hamiltonian operators\n",
    "        CONFIG: Dictionary with keys:\n",
    "            - 'L': number of qubits\n",
    "            - 'x_fields': bool (whether to include X rotations)\n",
    "            - 'y_fields': bool (whether to include Y rotations)\n",
    "            - 'z_fields': bool (whether to include Z rotations)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys: 'theta', 'rot_x', 'rot_y', 'rot_z' (only if active)\n",
    "    \"\"\"\n",
    "    predicted_params = {}\n",
    "\n",
    "    if params.ndim > 1:\n",
    "        params = params.squeeze() \n",
    "    \n",
    "    # Start index for slicing params\n",
    "    idx = 0\n",
    "    \n",
    "    # 1. Hamiltonian parameters (theta)\n",
    "    n_hamiltonian = len(OPS_LIST)\n",
    "    predicted_params['theta'] = params[idx:idx + n_hamiltonian]\n",
    "    idx += n_hamiltonian\n",
    "    \n",
    "    # 2. Rotation parameters based on configuration\n",
    "    L = CONFIG['L']\n",
    "    \n",
    "    if CONFIG.get('x_fields', False):\n",
    "        predicted_params['rot_x'] = params[idx:idx + L]\n",
    "        idx += L\n",
    "    \n",
    "    if CONFIG.get('y_fields', False):\n",
    "        predicted_params['rot_y'] = params[idx:idx + L]\n",
    "        idx += L\n",
    "    \n",
    "    if CONFIG.get('z_fields', False):\n",
    "        predicted_params['rot_z'] = params[idx:idx + L]\n",
    "        idx += L\n",
    "    \n",
    "    # Verify we used all parameters\n",
    "    if idx != params.shape[0]:\n",
    "        raise ValueError(f\"Parameter count mismatch. Expected {idx} parameters, \"\n",
    "                        f\"but model output has {params.shape[0]}\")\n",
    "    \n",
    "    return predicted_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd9b57",
   "metadata": {},
   "source": [
    "## Training algorithm\n",
    "\n",
    "Function that contains steps of one epoch\n",
    "\n",
    "1. Forward step of NN\n",
    "2. Simulation of physics (rotations + H evolution)\n",
    "3. Calculation of nll\n",
    "4. Backpropagation\n",
    "\n",
    "\n",
    "Function that loops for all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "6e81351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, params, n_epochs, single_qubit_probs, psi0, OPS_LIST, CONFIG, t_grid_fine, learning_rate, counts_shots):\n",
    "\n",
    "    #initialization\n",
    "    loss_history = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "   #Loss with initial parameters \n",
    "    # optimizer.zero_grad()\n",
    "    # psi_t = physics_computation(params, psi0, OPS_LIST, CONFIG, t_grid_fine)\n",
    "    # loss = nll(psi_t, counts_shots)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "    # loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "    for epoch_i in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: NN predicts Hamiltonian parameters\n",
    "        predicted_params = {}\n",
    "        output_params = model(single_qubit_probs)\n",
    "\n",
    "        predicted_params = create_parameter_dict(output_params, OPS_LIST, CONFIG)\n",
    "\n",
    "        #Dynamics of obtained parameters\n",
    "        psi_t = physics_computation(predicted_params, psi0, OPS_LIST, CONFIG, t_grid_fine)\n",
    "\n",
    "        #compute loss\n",
    "        loss = nll(psi_t, counts_shots)\n",
    "\n",
    "        #backpropagate and update optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "         \n",
    "    return model, predicted_params, psi_t, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/omichel/Desktop/qilimanjaro/projects/retech/retech_2025/tensorkrowch_version/config/MPS_learning_configuration.yaml\n",
      "\n",
      "============================================================\n",
      "LOADING DATA: L4_Chi_2_R50000_counts\n",
      "============================================================\n",
      "ZZ terms added to the Hamiltonian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omichel/.pyenv/versions/retech_env/lib/python3.12/site-packages/tensorkrowch/components.py:1506: UserWarning: `tensor` is being cropped to fit the shape of node \"stack_data_memory\" at non-batch edges\n",
      "  warnings.warn(f'`tensor` is being cropped to fit the shape of '\n",
      "/Users/omichel/.pyenv/versions/retech_env/lib/python3.12/site-packages/tensorkrowch/components.py:1348: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/python_variable_indexing.cpp:353.)\n",
      "  return tensor[index]\n"
     ]
    }
   ],
   "source": [
    "config_file = \"/Users/omichel/Desktop/qilimanjaro/projects/retech/retech_2025/tensorkrowch_version/config/MPS_learning_configuration.yaml\"\n",
    "\n",
    "#load configuration\n",
    "print(config_file)\n",
    "CONFIG = load_config(config_file)\n",
    "\n",
    "# Load data\n",
    "bitstrings, counts_shots = load_experimental_data(CONFIG)\n",
    "\n",
    "#Main parameters\n",
    "L = CONFIG['L']\n",
    "CHI = CONFIG['bond_dimension']\n",
    "inital_state_kind = CONFIG['initial_state_kind']\n",
    "dim = 2**L\n",
    "\n",
    "#Reshape data into local probabilities\n",
    "single_qubit_probs = local_probability_tensor(bitstrings, counts_shots)\n",
    "\n",
    "psi0 = prepare_initial_state(L, inital_state_kind)\n",
    "\n",
    "#Initialize and onfigure Hamiltonian Ansatz\n",
    "OPS_LIST = OperatorClass(L)\n",
    "\n",
    "OPS_LIST.add_operators('ZZ')\n",
    "\n",
    "NUM_COEFFICIENTS = len(OPS_LIST)\n",
    "\n",
    "#Initialize parameters\n",
    "torch.manual_seed(CONFIG[\"seed_init\"])\n",
    "\n",
    "theta_init = torch.rand(NUM_COEFFICIENTS, dtype=torch.float32, requires_grad=True)\n",
    "# Initialize NN\n",
    "NN_INPUT_DIM = L\n",
    "\n",
    "params = {\"theta\": theta_init}\n",
    "\n",
    "# Add rotation parameters for each enabled field type\n",
    "if CONFIG['x_fields']:\n",
    "    params[\"rot_x\"] = torch.rand(L, dtype=torch.float32, requires_grad=True)\n",
    "if CONFIG['y_fields']:\n",
    "    params[\"rot_y\"] = torch.rand(L, dtype=torch.float32, requires_grad=True)\n",
    "if CONFIG['z_fields']:\n",
    "    params[\"rot_z\"] = torch.rand(L, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Update NN output dimension\n",
    "NN_OUTPUT_DIM = NUM_COEFFICIENTS + sum(CONFIG[f'{axis}_fields'] for axis in ['x', 'y', 'z']) * L\n",
    "\n",
    "model = MPS_MLP(NN_INPUT_DIM, CHI, NN_OUTPUT_DIM, num_dims = []) #num_dims is for optional intermediate layers\n",
    "n_epochs = CONFIG['N_epochs']\n",
    "\n",
    "t_grid_fine = torch.arange(0.0, CONFIG[\"t_max\"] + CONFIG[\"dt\"]/2, CONFIG[\"dt\"])\n",
    "learning_rate = CONFIG['learning_rate']\n",
    "\n",
    "model, final_params, psi_final, loss = train_model(model, params, n_epochs, single_qubit_probs, psi0, OPS_LIST, CONFIG, t_grid_fine, learning_rate, counts_shots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "903c0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'theta': tensor([-0.2385, -0.1752,  0.5253], grad_fn=<SliceBackward0>), 'rot_x': tensor([-0.0557, -0.1260,  0.0406,  0.1731], grad_fn=<SliceBackward0>), 'rot_z': tensor([ 0.0161, -0.3199, -0.4579, -0.0341], grad_fn=<SliceBackward0>)}\n",
      "[2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725884914398193, 2.7725889682769775, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725884914398193, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725889682769775, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725889682769775, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984, 2.7725884914398193, 2.7725884914398193, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725884914398193, 2.7725887298583984, 2.7725884914398193, 2.7725884914398193, 2.7725887298583984, 2.7725884914398193, 2.7725884914398193, 2.7725887298583984, 2.7725887298583984, 2.7725887298583984]\n"
     ]
    }
   ],
   "source": [
    "print(final_params)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637049d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retech_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
