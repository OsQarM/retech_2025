{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e6e728",
   "metadata": {},
   "source": [
    "# MPS Hamiltonian Learning with TensorKrowch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d30c3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import tensorkrowch as tk\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ea603",
   "metadata": {},
   "source": [
    "### Things to do:\n",
    "\n",
    "This will be a pytorch version of the Hamiltonian learning problem, where the NN has a layer with an MPS structure from tensorkrowch. There are a few things that I need to sort out:\n",
    "1. This NN doesn't train from data, it starts from an ansatz and modifies it until it finds the optimal solution\n",
    "2. I still need to run the dynamics for every epoch, which consist on:\n",
    "    2.1. Taking initial state and applyting rotations in the X,Y and Z directions, with the option to customize which rotations I want to apply\n",
    "    2.2. Doing a time evolution of the resulting state under a Hamiltonian. The Hamiltonian contains only interaction terms, and it also must be customizable\n",
    "3. After running the Hamiltonian, we extract bitstring probabilities and compute nll loss function with the input data, which are the generated bitstrings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596daf3",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "551129c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    '''Load configuration from YAML file'''\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def load_experimental_data(config):\n",
    "    \"\"\"Load experimental/simulated data\"\"\"\n",
    "    N = config[\"L\"]\n",
    "    chi = config['bond_dimension']\n",
    "    T_max = config[\"t_max\"]\n",
    "    search_pattern = f\"../data/experimental_data_quantum_sampling_L{N}_Chi_{chi}_*_counts.csv\"\n",
    "    files = glob.glob(search_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No data found for L={N}\")\n",
    "\n",
    "    config_file = files[0]\n",
    "    file_core = config_file.replace(\".csv\", \"\").replace(\"../data/experimental_data_quantum_sampling_\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LOADING DATA: {file_core}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df_counts = pd.read_csv(f\"../data/experimental_data_quantum_sampling_{file_core}.csv\")\n",
    "        \n",
    "    # Remove leading single quote if present\n",
    "    if df_counts['bitstring'].astype(str).str.startswith(\"'\").all():\n",
    "        df_counts['bitstring'] = df_counts['bitstring'].str[1:]\n",
    "    \n",
    "    # Now extract values\n",
    "    bitstrings = df_counts['bitstring'].values.astype(str)\n",
    "    counts_shots = df_counts['count'].values.astype(np.int32)\n",
    "    \n",
    "    return bitstrings, counts_shots\n",
    "\n",
    "\n",
    "def local_probability_tensor(strings, counts):\n",
    "    '''Calculates probabilities of each qubit, returning vector of size L\n",
    "    containing the probs of each qubit being 0 or 1'''\n",
    "    L = len(strings[0])\n",
    "    total_counts = sum(counts)\n",
    "    \n",
    "    prob_matrix = torch.zeros((1, L, 2)) #First index is batch. Needed for feeding into NN\n",
    "    \n",
    "    for bitstring, count in zip(strings, counts):\n",
    "        for qubit in range(L):\n",
    "            bit_value = int(bitstring[qubit])\n",
    "            prob_matrix[0, qubit, bit_value] += count\n",
    "    \n",
    "    # Normalize by total counts\n",
    "    prob_matrix /= total_counts\n",
    "    \n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f031b6",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c36a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paulis(dtype=torch.complex64):\n",
    "    '''Creates single-qubit basis operators'''\n",
    "    sx = torch.tensor([[0., 1.], [1., 0.]], dtype=dtype)\n",
    "    sy = torch.tensor([[0., -1j], [1j, 0.]], dtype=dtype)\n",
    "    sz = torch.tensor([[1., 0.], [0., -1.]], dtype=dtype)\n",
    "    id2 = torch.eye(2, dtype=dtype)\n",
    "    return sx, sy, sz, id2\n",
    "\n",
    "def kron_n(ops):\n",
    "    '''Tensor product of a list of operators'''\n",
    "    out = ops[0]\n",
    "    for A in ops[1:]:\n",
    "        out = torch.kron(out, A)\n",
    "    return out\n",
    "\n",
    "\n",
    "def x_rotation(theta, dtype=torch.complex64):\n",
    "    sx = torch.tensor([[0., 1.], [1., 0.]], dtype=dtype)\n",
    "    return torch.matrix_exp(-1j * theta / 2 * sx)\n",
    "\n",
    "def y_rotation(theta, dtype=torch.complex64):\n",
    "    sy = torch.tensor([[0., -1j], [1j, 0.]], dtype=dtype)\n",
    "    return torch.matrix_exp(-1j * theta / 2 * sy)\n",
    "\n",
    "def z_rotation(theta, dtype=torch.complex64):\n",
    "    sz = torch.tensor([[1., 0.], [0., -1.]], dtype=dtype)\n",
    "    return torch.matrix_exp(-1j * theta / 2 * sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78840517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_initial_state(L, kind, dtype=torch.complex64):\n",
    "    \"\"\"Prepare initial quantum states for L qubits.\"\"\"\n",
    "    if kind == 'all_zeros':\n",
    "        psi0 = torch.zeros(2**L, dtype=dtype)\n",
    "        psi0[0] = 1.0\n",
    "        \n",
    "    elif kind == 'all_plus':\n",
    "        plus = torch.ones(2, dtype=dtype) / np.sqrt(2)\n",
    "        psi0 = plus\n",
    "        for _ in range(L - 1):\n",
    "            psi0 = torch.kron(psi0, plus)\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(f\"Initial state '{kind}' not recognized. \"\n",
    "                        f\"Use 'all_zeros' or 'all_plus'\")\n",
    "    return psi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f949bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperatorClass:\n",
    "    '''Class that contains a list of all the operator types the Hamiltonian will have\n",
    "       The operators will be applied to each qubit, and we will allow for the construction of any\n",
    "       combination of Pauli strings \n",
    "    '''\n",
    "    def __init__(self, L, dtype=torch.complex64):\n",
    "\n",
    "        self.L = L\n",
    "        self.dim = 2**L\n",
    "        self.pauli_basis = {}\n",
    "        self.pauli_basis['X'], self.pauli_basis['Y'], self.pauli_basis['Z'], self.pauli_basis['I'] = paulis(dtype)\n",
    "        self.operators = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.operators)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.operators[idx]\n",
    "    \n",
    "    def add_operators(self, pauli_string:str):\n",
    "        #e.g. 'X','Y','ZZ'\n",
    "        '''Adds one type of operator at a time. It loops through all the qubits, \n",
    "        and for each position does the tensor product of the whole chain, with the \n",
    "        required qubits substituted by the operators of the string'''\n",
    "\n",
    "        if len(pauli_string) > self.L:\n",
    "            raise ValueError(f\"Pauli string '{pauli_string}' longer than system size {self.L}\")\n",
    "        \n",
    "        if not all(char in 'XYZI' for char in pauli_string):\n",
    "            raise ValueError(f\"Invalid character in '{pauli_string}'. Use only X, Y, Z, I\")\n",
    "        \n",
    "        for i in range(self.L - len(pauli_string) + 1):\n",
    "                #Create identity operators for each qubit\n",
    "                ops = [self.pauli_basis['I']]*self.L\n",
    "                for j, char in enumerate(pauli_string):\n",
    "                     #Build string\n",
    "                     ops[i+j] = self.pauli_basis[char]\n",
    "                self.operators.append(kron_n(ops))\n",
    "        print(f\"{pauli_string} terms added to the Hamiltonian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11a645",
   "metadata": {},
   "source": [
    "## Manual NN\n",
    "\n",
    "These were the functions defined by Marcin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db2551a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(params, x):\n",
    "    h = x\n",
    "    for layer in params[:-1]:\n",
    "        h = np.tanh(h @ layer[\"W\"] + layer[\"b\"])\n",
    "    last = params[-1]\n",
    "    return h @ last[\"W\"] + last[\"b\"]\n",
    "\n",
    "\n",
    "def init_mlp_params(layer_sizes, scale=0.1):\n",
    "    params = []\n",
    "    for i, (m, n) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        # Initialize weights with scaled normal distribution\n",
    "        W = scale * torch.randn((m, n))\n",
    "        # Initialize biases to zero\n",
    "        b = torch.zeros((n,))\n",
    "        params.append({\"W\": W, \"b\": b})\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3dfc28",
   "metadata": {},
   "source": [
    "## Pytorch NN\n",
    "\n",
    "Equivalent of Marcin's functions but with Pytorch syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "394c583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Create linear layers\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "        \n",
    "        # Custom initialization (similar to your function)\n",
    "        self._initialize_parameters()\n",
    "    \n",
    "    def _initialize_parameters(self, scale=0.1):\n",
    "        \"\"\"Initialize weights with normal distribution and biases to zero.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=scale)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply all but last layer with tanh activation\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = torch.tanh(layer(x))\n",
    "        \n",
    "        # Last layer - linear only (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32e902",
   "metadata": {},
   "source": [
    "## TensorKrowch NN\n",
    "\n",
    "My pytorch NN integrating MPS layer from TensorKrowch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e09b1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, for input layer I should put number of possible bitstrings, and for output number of parameters to train. In the middle idk yet, but we'll see\n",
    "class MPS_MLP(nn.Module):\n",
    "    def __init__(self, L, chi, num_params, num_dims = []):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        layer_sizes = num_dims + [num_params]\n",
    "\n",
    "        # 1. MPS input layer: processes L×2 features → first hidden size (will be output size if no middle layers)\n",
    "        mps = tk.models.MPSLayer(\n",
    "            n_features=L,\n",
    "            in_dim=2,\n",
    "            out_dim=layer_sizes[0],  \n",
    "            bond_dim=chi\n",
    "        )\n",
    "        self.layers.append(mps)\n",
    "        \n",
    "        # 2. Middle layers (optional)\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            self.layers.append(nn.Linear(\n",
    "                layer_sizes[i],\n",
    "                layer_sizes[i+1]\n",
    "            ))\n",
    "        \n",
    "        if len(layer_sizes) > 1:\n",
    "        # 3. Final output layer: hidden → num_params\n",
    "            self.layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
    "        \n",
    "        # Custom initialization (similar to your function)\n",
    "        self._initialize_parameters()\n",
    "    \n",
    "    def _initialize_parameters(self, scale=0.1):\n",
    "        \"\"\"Initialize weights with normal distribution and biases to zero.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear): #MPSLayer initializes itself\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=scale)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply all but last layer with tanh activation\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = torch.tanh(layer(x))\n",
    "        \n",
    "        # Last layer - linear only (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43577c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_computation(params, psi0, OPS_LIST, t_grid):\n",
    "    \n",
    "    pass\n",
    "\n",
    "def nll():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd9b57",
   "metadata": {},
   "source": [
    "## Training algorithm\n",
    "\n",
    "Function that contains steps of one epoch\n",
    "\n",
    "1. Forward step of NN\n",
    "2. Simulation of physics (rotations + H evolution)\n",
    "3. Calculation of nll\n",
    "4. Backpropagation\n",
    "\n",
    "\n",
    "Function that loops for all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, params, n_epochs, single_qubit_probs, psi0, OPS_LIST, t_grid_fine, learning_rate, counts_shots):\n",
    "\n",
    "\n",
    "    loss_history = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #initialization\n",
    "    optimizer.zero_grad()\n",
    "    psi_t = physics_computation(params, psi0, OPS_LIST, t_grid_fine)\n",
    "    loss = nll(psi_t, counts_shots)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    for epoch_i in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: NN predicts Hamiltonian parameters\n",
    "        predicted_params = model(single_qubit_probs)\n",
    "\n",
    "        #Dynamics of obtained parameters\n",
    "        psi_t = physics_computation(predicted_params[0], psi0, OPS_LIST, t_grid_fine)\n",
    "\n",
    "        #compute loss\n",
    "        loss = nll(psi_t, counts_shots)\n",
    "\n",
    "        #backpropagate and update optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "         \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7741da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/omichel/Desktop/qilimanjaro/projects/retech/retech_2025/tensorkrowch_version/config/MPS_learning_configuration.yaml\n",
      "\n",
      "============================================================\n",
      "LOADING DATA: L4_Chi_2_R50000_counts\n",
      "============================================================\n",
      "ZZ terms added to the Hamiltonian\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3033, -0.3643,  0.7850, -0.1625, -0.2729, -0.0134,  0.1939,  0.1179,\n",
      "        -0.5936, -0.8166, -0.1648], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omichel/.pyenv/versions/retech_env/lib/python3.12/site-packages/tensorkrowch/components.py:1506: UserWarning: `tensor` is being cropped to fit the shape of node \"stack_data_memory\" at non-batch edges\n",
      "  warnings.warn(f'`tensor` is being cropped to fit the shape of '\n",
      "/Users/omichel/.pyenv/versions/retech_env/lib/python3.12/site-packages/tensorkrowch/components.py:1348: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/python_variable_indexing.cpp:353.)\n",
      "  return tensor[index]\n"
     ]
    }
   ],
   "source": [
    "config_file = \"/Users/omichel/Desktop/qilimanjaro/projects/retech/retech_2025/tensorkrowch_version/config/MPS_learning_configuration.yaml\"\n",
    "\n",
    "#load configuration\n",
    "print(config_file)\n",
    "CONFIG = load_config(config_file)\n",
    "\n",
    "# Load data\n",
    "bitstrings, counts_shots = load_experimental_data(CONFIG)\n",
    "\n",
    "#Main parameters\n",
    "L = CONFIG['L']\n",
    "CHI = CONFIG['bond_dimension']\n",
    "inital_state_kind = CONFIG['initial_state_kind']\n",
    "dim = 2**L\n",
    "\n",
    "#Reshape data into local probabilities\n",
    "single_qubit_probs = local_probability_tensor(bitstrings, counts_shots)\n",
    "\n",
    "psi0 = prepare_initial_state(L, inital_state_kind)\n",
    "\n",
    "#Initialize and onfigure Hamiltonian Ansatz\n",
    "OPS_LIST = OperatorClass(L)\n",
    "\n",
    "OPS_LIST.add_operators('ZZ')\n",
    "\n",
    "NUM_COEFFICIENTS = len(OPS_LIST)\n",
    "NUM_ROTATIONS = L * (CONFIG['x_fields'] + CONFIG['y_fields'] + CONFIG['z_fields'])\n",
    "\n",
    "#Initialize parameters\n",
    "torch.manual_seed(CONFIG[\"seed_init\"])\n",
    "\n",
    "theta_init = torch.rand(NUM_COEFFICIENTS, dtype=torch.float32)\n",
    "rot_init = torch.rand(NUM_ROTATIONS, dtype=torch.float32)\n",
    "\n",
    "# Initialize NN\n",
    "NN_INPUT_DIM = L\n",
    "NN_OUTPUT_DIM = NUM_COEFFICIENTS + NUM_ROTATIONS\n",
    "\n",
    "model = MPS_MLP(NN_INPUT_DIM, CHI, NN_OUTPUT_DIM, num_dims = []) #num_dims is for optional intermediate layers\n",
    "params = {\"theta\": theta_init, \"rot\": rot_init}\n",
    "n_epochs = CONFIG['N_epochs']\n",
    "\n",
    "t_grid_fine = torch.arange(0.0, CONFIG[\"t_max\"] + CONFIG[\"dt\"]/2, CONFIG[\"dt\"])\n",
    "learning_rate = CONFIG['learning_rate']\n",
    "\n",
    "train_model(model, params, n_epochs, single_qubit_probs, theta_init, rot_init, psi0, OPS_LIST, t_grid_fine, learning_rate, counts_shots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c0ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637049d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retech_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
